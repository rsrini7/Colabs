{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODZcP84kAi00pd00Z+qQIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsrini7/Colabs/blob/main/llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kKDCtliswabt"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index chromadb llama-index-vector-stores-chroma llama-index-embeddings-huggingface sentence-transformers llama-index-llms-openai litellm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llamaindex_rag_openrouter_colab_litellm.py\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from typing import Optional, Any, AsyncGenerator, Generator\n",
        "\n",
        "# --- LlamaIndex Imports ---\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    Settings,\n",
        "    Document\n",
        ")\n",
        "from llama_index.core.llms import CustomLLM, CompletionResponse, CompletionResponseGen, LLMMetadata\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# --- Other Necessary Imports ---\n",
        "import chromadb\n",
        "import litellm\n",
        "\n",
        "# --- Configuration & Constants ---\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "DATA_DIR = \"./data\"\n",
        "SAMPLE_FILE_NAME = \"sample.txt\"\n",
        "OPENROUTER_LITELLM_MODEL_STRING = \"openrouter/openai/gpt-3.5-turbo\" # Or your preferred OpenRouter model\n",
        "DB_PATH = './db_chroma_llamaindex_openrouter_litellm'\n",
        "COLLECTION_NAME = \"llamaindex_rag_openrouter_colab_litellm\"\n",
        "\n",
        "# --- Helper: Setup Logging (Optional) ---\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger('litellm').setLevel(logging.INFO) # To see LiteLLM logs\n",
        "\n",
        "# --- Custom LLM Class using LiteLLM ---\n",
        "class LiteLLMCustom(CustomLLM):\n",
        "    model_string_for_litellm: str = OPENROUTER_LITELLM_MODEL_STRING\n",
        "    num_output: int = 512\n",
        "\n",
        "    _model_name_internal: str\n",
        "    _actual_context_window: int\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_string_for_litellm: Optional[str] = None,\n",
        "                 num_output: Optional[int] = None,\n",
        "                 callback_manager: Optional[CallbackManager] = None,\n",
        "                 **kwargs: Any):\n",
        "        init_data = {}\n",
        "        if model_string_for_litellm is not None:\n",
        "            init_data[\"model_string_for_litellm\"] = model_string_for_litellm\n",
        "        if num_output is not None:\n",
        "            init_data[\"num_output\"] = num_output\n",
        "        if callback_manager is not None:\n",
        "            init_data[\"callback_manager\"] = callback_manager\n",
        "        init_data.update(kwargs)\n",
        "        super().__init__(**init_data)\n",
        "        self._model_name_internal = self.model_string_for_litellm\n",
        "        self._actual_context_window = self._get_model_info(self.model_string_for_litellm)\n",
        "\n",
        "    def _get_model_info(self, model_name_param: str) -> int:\n",
        "        try:\n",
        "            info = litellm.get_model_info(model_name_param)\n",
        "            if info and 'max_input_tokens' in info and info['max_input_tokens'] is not None:\n",
        "                return int(info['max_input_tokens'])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not get model info for {model_name_param} from LiteLLM: {e}. Using fallback 4096.\")\n",
        "        return 4096\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        return LLMMetadata(\n",
        "            context_window=self._actual_context_window,\n",
        "            num_output=self.num_output,\n",
        "            model_name=self._model_name_internal,\n",
        "        )\n",
        "\n",
        "    def _prepare_litellm_kwargs(self, **kwargs) -> dict:\n",
        "        allowed_litellm_keys = {\"temperature\", \"max_tokens\", \"top_p\", \"stop\", \"presence_penalty\", \"frequency_penalty\", \"seed\"}\n",
        "        return {key: value for key, value in kwargs.items() if key in allowed_litellm_keys}\n",
        "\n",
        "    def complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response = litellm.completion(\n",
        "            model=self.model_string_for_litellm, messages=messages,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        text_response = response.choices[0].message.content or \"\"\n",
        "        return CompletionResponse(text=text_response, raw=response.model_dump()) # UPDATED\n",
        "\n",
        "    async def acomplete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response = await litellm.acompletion(\n",
        "            model=self.model_string_for_litellm, messages=messages,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        text_response = response.choices[0].message.content or \"\"\n",
        "        return CompletionResponse(text=text_response, raw=response.model_dump()) # UPDATED\n",
        "\n",
        "    def stream_complete(self, prompt: str, formatted: bool = False, **kwargs) -> Generator[CompletionResponse, None, None]:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response_stream = litellm.completion(\n",
        "            model=self.model_string_for_litellm, messages=messages, stream=True,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        content_so_far = \"\"\n",
        "        for chunk in response_stream:\n",
        "            delta = \"\"\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta = chunk.choices[0].delta.content or \"\"\n",
        "            if delta:\n",
        "                content_so_far += delta\n",
        "                yield CompletionResponse(text=content_so_far, delta=delta, raw=chunk.model_dump()) # UPDATED\n",
        "\n",
        "    async def astream_complete(self, prompt: str, formatted: bool = False, **kwargs) -> AsyncGenerator[CompletionResponse, None]:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response_stream = await litellm.acompletion(\n",
        "            model=self.model_string_for_litellm, messages=messages, stream=True,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        content_so_far = \"\"\n",
        "        async for chunk in response_stream:\n",
        "            delta = \"\"\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta = chunk.choices[0].delta.content or \"\"\n",
        "            if delta:\n",
        "                content_so_far += delta\n",
        "                yield CompletionResponse(text=content_so_far, delta=delta, raw=chunk.model_dump()) # UPDATED\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "def main():\n",
        "    print(\"--- Starting LlamaIndex RAG with OpenRouter via LiteLLM ---\")\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        os.environ[\"OPENROUTER_API_KEY\"] = openrouter_api_key\n",
        "        print(\"OpenRouter API Key loaded from Colab Secrets.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ERROR: OPENROUTER_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load OpenRouter API Key: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    sample_file_path = os.path.join(DATA_DIR, SAMPLE_FILE_NAME)\n",
        "    if not os.path.exists(DATA_DIR): os.makedirs(DATA_DIR)\n",
        "    if not os.path.exists(sample_file_path):\n",
        "        with open(sample_file_path, \"w\") as f:\n",
        "            f.write(\"\"\"The history of AI began in antiquity, with myths and stories.\n",
        "Modern AI started in the 1950s with Alan Turing.\n",
        "Key developments include machine learning and deep learning.\n",
        "Large Language Models (LLMs) like GPT-4 are a significant advancement.\n",
        "Frameworks like LlamaIndex help build LLM applications.\n",
        "OpenRouter provides access to many different LLMs.\n",
        "Vector databases are essential for semantic search in RAG.\n",
        "\"\"\")\n",
        "        print(f\"Created dummy sample file: '{sample_file_path}'\")\n",
        "\n",
        "    print(f\"\\nConfiguring LLM: Custom LiteLLM Wrapper with model '{OPENROUTER_LITELLM_MODEL_STRING}'\")\n",
        "    Settings.llm = LiteLLMCustom(model_string_for_litellm=OPENROUTER_LITELLM_MODEL_STRING)\n",
        "    print(f\"LLM configured. Context window: {Settings.llm.metadata.context_window}, Output size: {Settings.llm.metadata.num_output}\")\n",
        "\n",
        "    print(f\"\\nConfiguring Embedding Model: '{EMBED_MODEL_NAME}'\")\n",
        "    try:\n",
        "        Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)\n",
        "        print(\"Embedding model configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load HuggingFace embedding model '{EMBED_MODEL_NAME}': {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 1. Ingesting Data ---\")\n",
        "    try:\n",
        "        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "        if not documents:\n",
        "            print(f\"Warning: No documents loaded from '{DATA_DIR}'.\")\n",
        "            sys.exit(1)\n",
        "        print(f\"Loaded {len(documents)} document(s) from '{DATA_DIR}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during document loading: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 2. Storing in Vector Database (ChromaDB) ---\")\n",
        "    try:\n",
        "        chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
        "        chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)\n",
        "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        print(f\"ChromaDB setup: collection '{COLLECTION_NAME}' at '{DB_PATH}'. Initial count: {chroma_collection.count()}\")\n",
        "        print(\"Building or loading index...\")\n",
        "        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "        print(f\"Index built/loaded. Documents in Chroma collection now: {chroma_collection.count()}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during vector store or indexing setup: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 3. Performing Explicit Search ---\")\n",
        "    query = \"What are key developments in AI?\"\n",
        "    try:\n",
        "        retriever = index.as_retriever(similarity_top_k=2)\n",
        "        retrieved_nodes = retriever.retrieve(query)\n",
        "        print(f\"Search query: '{query}'\")\n",
        "        print(f\"Found {len(retrieved_nodes)} relevant node(s):\")\n",
        "        for i, node_with_score in enumerate(retrieved_nodes):\n",
        "            print(f\"  Result {i+1} (Score: {node_with_score.score:.4f}): {node_with_score.node.get_content()[:100].strip()}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during retrieval: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 4. Generating Answer with LLM ---\")\n",
        "    try:\n",
        "        query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "        print(f\"Querying LLM with: '{query}'\")\n",
        "        response = query_engine.query(query)\n",
        "        print(f\"\\nLLM Answer for '{query}':\")\n",
        "        print(f\"Answer: {response.response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during query engine execution or LLM call: {e}\")\n",
        "\n",
        "    print(\"\\n--- LlamaIndex RAG with OpenRouter via LiteLLM Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb7Dy7dVzHbS",
        "outputId": "a7f50f71-d668-400c-ce74-d18982a9cdb1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting LlamaIndex RAG with OpenRouter via LiteLLM ---\n",
            "OpenRouter API Key loaded from Colab Secrets.\n",
            "\n",
            "Configuring LLM: Custom LiteLLM Wrapper with model 'openrouter/openai/gpt-3.5-turbo'\n",
            "LLM configured. Context window: 4096, Output size: 512\n",
            "\n",
            "Configuring Embedding Model: 'sentence-transformers/all-MiniLM-L6-v2'\n",
            "Embedding model configured successfully.\n",
            "\n",
            "--- 1. Ingesting Data ---\n",
            "Loaded 1 document(s) from './data'.\n",
            "\n",
            "--- 2. Storing in Vector Database (ChromaDB) ---\n",
            "ChromaDB setup: collection 'llamaindex_rag_openrouter_colab_litellm' at './db_chroma_llamaindex_openrouter_litellm'. Initial count: 2\n",
            "Building or loading index...\n",
            "Index built/loaded. Documents in Chroma collection now: 3.\n",
            "\n",
            "--- 3. Performing Explicit Search ---\n",
            "Search query: 'What are key developments in AI?'\n",
            "Found 1 relevant node(s):\n",
            "  Result 1 (Score: 0.3383): # sample.txt\n",
            "The history of AI began in antiquity, with myths, stories and rumors of artificial bein...\n",
            "\n",
            "--- 4. Generating Answer with LLM ---\n",
            "Querying LLM with: 'What are key developments in AI?'\n",
            "\n",
            "LLM Answer for 'What are key developments in AI?':\n",
            "Answer: Key developments in AI include the rise of machine learning in the 2000s and deep learning in the 2010s.\n",
            "\n",
            "--- LlamaIndex RAG with OpenRouter via LiteLLM Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AVLZwKP0vVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}