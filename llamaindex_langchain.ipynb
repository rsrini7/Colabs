{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3KsIVINjPjwEYRkk9ZlQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsrini7/Colabs/blob/main/llamaindex_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kKDCtliswabt"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index chromadb llama-index-vector-stores-chroma llama-index-embeddings-huggingface sentence-transformers llama-index-llms-openai litellm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llamaindex_rag_openrouter_colab_litellm.py\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from typing import Optional, Any, AsyncGenerator, Generator\n",
        "\n",
        "# --- LlamaIndex Imports ---\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    Settings,\n",
        "    Document\n",
        ")\n",
        "from llama_index.core.llms import CustomLLM, CompletionResponse, CompletionResponseGen, LLMMetadata\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# --- Other Necessary Imports ---\n",
        "import chromadb\n",
        "import litellm\n",
        "\n",
        "# --- Configuration & Constants ---\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "DATA_DIR = \"./data\"\n",
        "SAMPLE_FILE_NAME = \"sample.txt\"\n",
        "OPENROUTER_LITELLM_MODEL_STRING = \"openrouter/openai/gpt-3.5-turbo\" # Or your preferred OpenRouter model\n",
        "DB_PATH = './db_chroma_llamaindex_openrouter_litellm'\n",
        "COLLECTION_NAME = \"llamaindex_rag_openrouter_colab_litellm\"\n",
        "\n",
        "# --- Helper: Setup Logging (Optional) ---\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger('litellm').setLevel(logging.INFO) # To see LiteLLM logs\n",
        "\n",
        "# --- Custom LLM Class using LiteLLM ---\n",
        "class LiteLLMCustom(CustomLLM):\n",
        "    model_string_for_litellm: str = OPENROUTER_LITELLM_MODEL_STRING\n",
        "    num_output: int = 512\n",
        "\n",
        "    _model_name_internal: str\n",
        "    _actual_context_window: int\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_string_for_litellm: Optional[str] = None,\n",
        "                 num_output: Optional[int] = None,\n",
        "                 callback_manager: Optional[CallbackManager] = None,\n",
        "                 **kwargs: Any):\n",
        "        init_data = {}\n",
        "        if model_string_for_litellm is not None:\n",
        "            init_data[\"model_string_for_litellm\"] = model_string_for_litellm\n",
        "        if num_output is not None:\n",
        "            init_data[\"num_output\"] = num_output\n",
        "        if callback_manager is not None:\n",
        "            init_data[\"callback_manager\"] = callback_manager\n",
        "        init_data.update(kwargs)\n",
        "        super().__init__(**init_data)\n",
        "        self._model_name_internal = self.model_string_for_litellm\n",
        "        self._actual_context_window = self._get_model_info(self.model_string_for_litellm)\n",
        "\n",
        "    def _get_model_info(self, model_name_param: str) -> int:\n",
        "        try:\n",
        "            info = litellm.get_model_info(model_name_param)\n",
        "            if info and 'max_input_tokens' in info and info['max_input_tokens'] is not None:\n",
        "                return int(info['max_input_tokens'])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not get model info for {model_name_param} from LiteLLM: {e}. Using fallback 4096.\")\n",
        "        return 4096\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        return LLMMetadata(\n",
        "            context_window=self._actual_context_window,\n",
        "            num_output=self.num_output,\n",
        "            model_name=self._model_name_internal,\n",
        "        )\n",
        "\n",
        "    def _prepare_litellm_kwargs(self, **kwargs) -> dict:\n",
        "        allowed_litellm_keys = {\"temperature\", \"max_tokens\", \"top_p\", \"stop\", \"presence_penalty\", \"frequency_penalty\", \"seed\"}\n",
        "        return {key: value for key, value in kwargs.items() if key in allowed_litellm_keys}\n",
        "\n",
        "    def complete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response = litellm.completion(\n",
        "            model=self.model_string_for_litellm, messages=messages,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        text_response = response.choices[0].message.content or \"\"\n",
        "        return CompletionResponse(text=text_response, raw=response.model_dump()) # UPDATED\n",
        "\n",
        "    async def acomplete(self, prompt: str, formatted: bool = False, **kwargs) -> CompletionResponse:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response = await litellm.acompletion(\n",
        "            model=self.model_string_for_litellm, messages=messages,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        text_response = response.choices[0].message.content or \"\"\n",
        "        return CompletionResponse(text=text_response, raw=response.model_dump()) # UPDATED\n",
        "\n",
        "    def stream_complete(self, prompt: str, formatted: bool = False, **kwargs) -> Generator[CompletionResponse, None, None]:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response_stream = litellm.completion(\n",
        "            model=self.model_string_for_litellm, messages=messages, stream=True,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        content_so_far = \"\"\n",
        "        for chunk in response_stream:\n",
        "            delta = \"\"\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta = chunk.choices[0].delta.content or \"\"\n",
        "            if delta:\n",
        "                content_so_far += delta\n",
        "                yield CompletionResponse(text=content_so_far, delta=delta, raw=chunk.model_dump()) # UPDATED\n",
        "\n",
        "    async def astream_complete(self, prompt: str, formatted: bool = False, **kwargs) -> AsyncGenerator[CompletionResponse, None]:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        litellm_call_kwargs = self._prepare_litellm_kwargs(**kwargs)\n",
        "        response_stream = await litellm.acompletion(\n",
        "            model=self.model_string_for_litellm, messages=messages, stream=True,\n",
        "            api_key=os.getenv(\"OPENROUTER_API_KEY\"), **litellm_call_kwargs\n",
        "        )\n",
        "        content_so_far = \"\"\n",
        "        async for chunk in response_stream:\n",
        "            delta = \"\"\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta = chunk.choices[0].delta.content or \"\"\n",
        "            if delta:\n",
        "                content_so_far += delta\n",
        "                yield CompletionResponse(text=content_so_far, delta=delta, raw=chunk.model_dump()) # UPDATED\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "def main():\n",
        "    print(\"--- Starting LlamaIndex RAG with OpenRouter via LiteLLM ---\")\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        os.environ[\"OPENROUTER_API_KEY\"] = openrouter_api_key\n",
        "        print(\"OpenRouter API Key loaded from Colab Secrets.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ERROR: OPENROUTER_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load OpenRouter API Key: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    sample_file_path = os.path.join(DATA_DIR, SAMPLE_FILE_NAME)\n",
        "    if not os.path.exists(DATA_DIR): os.makedirs(DATA_DIR)\n",
        "    if not os.path.exists(sample_file_path):\n",
        "        with open(sample_file_path, \"w\") as f:\n",
        "            f.write(\"\"\"The history of AI began in antiquity, with myths and stories.\n",
        "Modern AI started in the 1950s with Alan Turing.\n",
        "Key developments include machine learning and deep learning.\n",
        "Large Language Models (LLMs) like GPT-4 are a significant advancement.\n",
        "Frameworks like LlamaIndex help build LLM applications.\n",
        "OpenRouter provides access to many different LLMs.\n",
        "Vector databases are essential for semantic search in RAG.\n",
        "\"\"\")\n",
        "        print(f\"Created dummy sample file: '{sample_file_path}'\")\n",
        "\n",
        "    print(f\"\\nConfiguring LLM: Custom LiteLLM Wrapper with model '{OPENROUTER_LITELLM_MODEL_STRING}'\")\n",
        "    Settings.llm = LiteLLMCustom(model_string_for_litellm=OPENROUTER_LITELLM_MODEL_STRING)\n",
        "    print(f\"LLM configured. Context window: {Settings.llm.metadata.context_window}, Output size: {Settings.llm.metadata.num_output}\")\n",
        "\n",
        "    print(f\"\\nConfiguring Embedding Model: '{EMBED_MODEL_NAME}'\")\n",
        "    try:\n",
        "        Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)\n",
        "        print(\"Embedding model configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load HuggingFace embedding model '{EMBED_MODEL_NAME}': {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 1. Ingesting Data ---\")\n",
        "    try:\n",
        "        documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
        "        if not documents:\n",
        "            print(f\"Warning: No documents loaded from '{DATA_DIR}'.\")\n",
        "            sys.exit(1)\n",
        "        print(f\"Loaded {len(documents)} document(s) from '{DATA_DIR}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during document loading: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 2. Storing in Vector Database (ChromaDB) ---\")\n",
        "    try:\n",
        "        chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
        "        chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)\n",
        "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        print(f\"ChromaDB setup: collection '{COLLECTION_NAME}' at '{DB_PATH}'. Initial count: {chroma_collection.count()}\")\n",
        "        print(\"Building or loading index...\")\n",
        "        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "        print(f\"Index built/loaded. Documents in Chroma collection now: {chroma_collection.count()}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during vector store or indexing setup: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 3. Performing Explicit Search ---\")\n",
        "    query = \"What are key developments in AI?\"\n",
        "    try:\n",
        "        retriever = index.as_retriever(similarity_top_k=2)\n",
        "        retrieved_nodes = retriever.retrieve(query)\n",
        "        print(f\"Search query: '{query}'\")\n",
        "        print(f\"Found {len(retrieved_nodes)} relevant node(s):\")\n",
        "        for i, node_with_score in enumerate(retrieved_nodes):\n",
        "            print(f\"  Result {i+1} (Score: {node_with_score.score:.4f}): {node_with_score.node.get_content()[:100].strip()}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during retrieval: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\n--- 4. Generating Answer with LLM ---\")\n",
        "    try:\n",
        "        query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "        print(f\"Querying LLM with: '{query}'\")\n",
        "        response = query_engine.query(query)\n",
        "        print(f\"\\nLLM Answer for '{query}':\")\n",
        "        print(f\"Answer: {response.response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during query engine execution or LLM call: {e}\")\n",
        "\n",
        "    print(\"\\n--- LlamaIndex RAG with OpenRouter via LiteLLM Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb7Dy7dVzHbS",
        "outputId": "1b5f4ae1-1945-40b5-f261-bdfa6056e178"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting LlamaIndex RAG with OpenRouter via LiteLLM ---\n",
            "OpenRouter API Key loaded from Colab Secrets.\n",
            "\n",
            "Configuring LLM: Custom LiteLLM Wrapper with model 'openrouter/openai/gpt-3.5-turbo'\n",
            "LLM configured. Context window: 4096, Output size: 512\n",
            "\n",
            "Configuring Embedding Model: 'sentence-transformers/all-MiniLM-L6-v2'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model configured successfully.\n",
            "\n",
            "--- 1. Ingesting Data ---\n",
            "Loaded 1 document(s) from './data'.\n",
            "\n",
            "--- 2. Storing in Vector Database (ChromaDB) ---\n",
            "ChromaDB setup: collection 'llamaindex_rag_openrouter_colab_litellm' at './db_chroma_llamaindex_openrouter_litellm'. Initial count: 3\n",
            "Building or loading index...\n",
            "Index built/loaded. Documents in Chroma collection now: 4.\n",
            "\n",
            "--- 3. Performing Explicit Search ---\n",
            "Search query: 'What are key developments in AI?'\n",
            "Found 1 relevant node(s):\n",
            "  Result 1 (Score: 0.3383): # sample.txt\n",
            "The history of AI began in antiquity, with myths, stories and rumors of artificial bein...\n",
            "\n",
            "--- 4. Generating Answer with LLM ---\n",
            "Querying LLM with: 'What are key developments in AI?'\n",
            "\n",
            "LLM Answer for 'What are key developments in AI?':\n",
            "Answer: Key developments in AI include the rise of machine learning in the 2000s and deep learning in the 2010s.\n",
            "\n",
            "--- LlamaIndex RAG with OpenRouter via LiteLLM Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-core sentence-transformers chromadb litellm --quiet"
      ],
      "metadata": {
        "id": "6AVLZwKP0vVq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# langchain_rag_openrouter_litellm.py\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from typing import Any, List, Mapping, Optional, Dict, Union, cast, AsyncIterator, Iterator\n",
        "\n",
        "# --- Langchain Imports ---\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun\n",
        "from langchain_core.outputs import GenerationChunk, Generation\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# --- Other Necessary Imports ---\n",
        "import litellm\n",
        "\n",
        "# --- Configuration & Constants ---\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "DATA_DIR = \"./data_langchain\" # Use a different data directory to avoid conflicts\n",
        "SAMPLE_FILE_NAME = \"sample_langchain.txt\"\n",
        "OPENROUTER_LITELLM_MODEL_STRING = \"openrouter/openai/gpt-3.5-turbo\" # Or your preferred OpenRouter model\n",
        "DB_PATH_LANGCHAIN = './db_chroma_langchain_openrouter_litellm'\n",
        "# COLLECTION_NAME_LANGCHAIN = \"langchain_rag_openrouter_litellm\" # Chroma handles this internally based on persist_directory\n",
        "\n",
        "# --- Helper: Setup Logging (Optional) ---\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger('litellm').setLevel(logging.INFO) # To see LiteLLM logs\n",
        "\n",
        "# --- Custom Langchain LLM Class using LiteLLM ---\n",
        "class LiteLLMWrapperForLangchain(LLM):\n",
        "    \"\"\"\n",
        "    Custom Langchain LLM Wrapper for LiteLLM.\n",
        "    \"\"\"\n",
        "    model_name: str = OPENROUTER_LITELLM_MODEL_STRING\n",
        "    \"\"\"The model name to pass to litellm.completion.\"\"\"\n",
        "\n",
        "    temperature: float = 0.0\n",
        "    \"\"\"The temperature to use for the completion.\"\"\"\n",
        "\n",
        "    max_tokens: Optional[int] = 512 # Max tokens for the *output*\n",
        "    \"\"\"The maximum number of tokens to generate.\"\"\"\n",
        "\n",
        "    top_p: float = 1.0\n",
        "    \"\"\"The top-p value to use for the completion.\"\"\"\n",
        "\n",
        "    litellm_kwargs: Optional[Dict[str, Any]] = None\n",
        "    \"\"\"Additional keyword arguments to pass to litellm.completion.\"\"\"\n",
        "\n",
        "    streaming: bool = False\n",
        "    \"\"\"Whether to stream the output.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"litellm_langchain_wrapper\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"streaming\": self.streaming,\n",
        "            **(self.litellm_kwargs or {}),\n",
        "        }\n",
        "\n",
        "    def _prepare_litellm_call_kwargs(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        kwargs = self.litellm_kwargs or {}\n",
        "        kwargs[\"model\"] = self.model_name\n",
        "        kwargs[\"temperature\"] = self.temperature\n",
        "        if self.max_tokens is not None: # LiteLLM uses max_tokens for output tokens\n",
        "             kwargs[\"max_tokens\"] = self.max_tokens\n",
        "        kwargs[\"top_p\"] = self.top_p\n",
        "        if stop:\n",
        "            kwargs[\"stop\"] = stop\n",
        "        kwargs[\"api_key\"] = os.getenv(\"OPENROUTER_API_KEY\") # LiteLLM can pick this up\n",
        "        return kwargs\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call out to LiteLLM's completion endpoint.\"\"\"\n",
        "        if self.streaming: # Langchain handles streaming via _stream or _astream\n",
        "            # This _call method is for non-streaming. If streaming is true,\n",
        "            # it implies the user might have set it expecting streaming from _generate.\n",
        "            # For simplicity here, we'll just make a non-streaming call if _call is invoked.\n",
        "            # A more robust implementation would raise an error or adapt.\n",
        "            pass\n",
        "\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs) # Allow overriding with call-specific kwargs\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        response = litellm.completion(messages=messages, **call_kwargs)\n",
        "        return response.choices[0].message.content or \"\"\n",
        "\n",
        "    async def _acall(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Async call out to LiteLLM's completion endpoint.\"\"\"\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs)\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        response = await litellm.acompletion(messages=messages, **call_kwargs)\n",
        "        return response.choices[0].message.content or \"\"\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[GenerationChunk]:\n",
        "        \"\"\"Stream responses from LiteLLM.\"\"\"\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs)\n",
        "        call_kwargs[\"stream\"] = True\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        for chunk in litellm.completion(messages=messages, **call_kwargs):\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta_content = chunk.choices[0].delta.content\n",
        "                if delta_content:\n",
        "                    yield GenerationChunk(text=delta_content)\n",
        "                    if run_manager:\n",
        "                        run_manager.on_llm_new_token(delta_content)\n",
        "\n",
        "    async def _astream(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> AsyncIterator[GenerationChunk]:\n",
        "        \"\"\"Async stream responses from LiteLLM.\"\"\"\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs)\n",
        "        call_kwargs[\"stream\"] = True\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        async for chunk in await litellm.acompletion(messages=messages, **call_kwargs):\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta_content = chunk.choices[0].delta.content\n",
        "                if delta_content:\n",
        "                    yield GenerationChunk(text=delta_content)\n",
        "                    if run_manager:\n",
        "                        await run_manager.on_llm_new_token(delta_content)\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "def main():\n",
        "    print(\"--- Starting Langchain RAG with OpenRouter via LiteLLM ---\")\n",
        "\n",
        "    # 0. Setup: API Keys and Sample Data\n",
        "    # litellm.set_verbose = True # Uncomment for verbose LiteLLM logs\n",
        "\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        os.environ[\"OPENROUTER_API_KEY\"] = openrouter_api_key # For LiteLLM\n",
        "        print(\"OpenRouter API Key loaded from Colab Secrets.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ERROR: OPENROUTER_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load OpenRouter API Key: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    sample_file_path = os.path.join(DATA_DIR, SAMPLE_FILE_NAME)\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "    if not os.path.exists(sample_file_path):\n",
        "        with open(sample_file_path, \"w\") as f:\n",
        "            f.write(\"\"\"Langchain is a framework for developing applications powered by language models.\n",
        "It provides modular components for building complex chains and agents.\n",
        "Key features include document loaders, text splitters, vector stores, and LLM wrappers.\n",
        "This example uses Langchain with OpenRouter via LiteLLM for RAG.\n",
        "Retrieval Augmented Generation enhances LLM responses with external data.\n",
        "\"\"\")\n",
        "        print(f\"Created dummy sample file: '{sample_file_path}'\")\n",
        "\n",
        "    # Configure Langchain Components\n",
        "    print(f\"\\nConfiguring LLM: Langchain LiteLLM Wrapper with model '{OPENROUTER_LITELLM_MODEL_STRING}'\")\n",
        "    llm = LiteLLMWrapperForLangchain(\n",
        "        model_name=OPENROUTER_LITELLM_MODEL_STRING,\n",
        "        temperature=0.0,\n",
        "        max_tokens=256 # Max output tokens for the LLM response\n",
        "    )\n",
        "    print(\"LLM configured.\")\n",
        "\n",
        "    print(f\"\\nConfiguring Embedding Model: '{EMBED_MODEL_NAME}'\")\n",
        "    try:\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
        "        print(\"Embedding model configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load HuggingFace embedding model '{EMBED_MODEL_NAME}': {e}\")\n",
        "        print(\"Ensure 'pip install sentence-transformers' has been run.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 1. Ingest Text File\n",
        "    print(\"\\n--- 1. Ingesting Data ---\")\n",
        "    try:\n",
        "        # Using DirectoryLoader to load all .txt files in the directory\n",
        "        loader = DirectoryLoader(DATA_DIR, glob=f\"**/{SAMPLE_FILE_NAME}\", loader_cls=TextLoader, show_progress=True)\n",
        "        documents = loader.load()\n",
        "        if not documents:\n",
        "            print(f\"Warning: No documents loaded from '{DATA_DIR}'. Ensure '{SAMPLE_FILE_NAME}' exists.\")\n",
        "            sys.exit(1)\n",
        "        print(f\"Loaded {len(documents)} document(s). Total characters: {sum(len(doc.page_content) for doc in documents)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during document loading: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(texts)} chunks.\")\n",
        "\n",
        "    # 2. Store Contents in a Vector Database (ChromaDB)\n",
        "    print(\"\\n--- 2. Storing in Vector Database (ChromaDB) ---\")\n",
        "    try:\n",
        "        print(f\"Initializing Chroma vector store at '{DB_PATH_LANGCHAIN}'...\")\n",
        "        # If the directory exists and has data, Chroma will load it.\n",
        "        # For a fresh run, you might want to delete the DB_PATH_LANGCHAIN directory.\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=texts,\n",
        "            embedding=embeddings,\n",
        "            persist_directory=DB_PATH_LANGCHAIN\n",
        "        )\n",
        "        vectorstore.persist() # Ensure persistence\n",
        "        print(f\"Vector store created/loaded. Collection count (approx): {vectorstore._collection.count()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during vector store setup: {e}\")\n",
        "        # If error is \"Invalid dimension\" check embedding model output vs Chroma expectations.\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 3. Perform a Search Operation (via Retriever)\n",
        "    print(\"\\n--- 3. Performing Explicit Search (Retriever) ---\")\n",
        "    query = \"What is Langchain?\"\n",
        "    try:\n",
        "        # Load from disk if needed (e.g., in a separate run after ingestion)\n",
        "        # vectorstore = Chroma(persist_directory=DB_PATH_LANGCHAIN, embedding_function=embeddings)\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) # Get top 2 results\n",
        "        retrieved_docs = retriever.invoke(query) # Langchain uses 'invoke'\n",
        "\n",
        "        print(f\"Search query: '{query}'\")\n",
        "        print(f\"Found {len(retrieved_docs)} relevant document chunk(s):\")\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            print(f\"  Result {i+1} (Metadata: {doc.metadata}): {doc.page_content[:150].strip()}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during retrieval: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 4. Pass Search Results to LLM for Generating Answers (RetrievalQA Chain)\n",
        "    print(\"\\n--- 4. Generating Answer with LLM using RetrievalQA Chain ---\")\n",
        "    try:\n",
        "        # Define a prompt template (optional, but good practice)\n",
        "        prompt_template_str = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Keep the answer concise and based *only* on the provided context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "        QA_PROMPT = PromptTemplate(\n",
        "            template=prompt_template_str, input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\", # \"stuff\" puts all context into the prompt\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True, # Optionally return source documents\n",
        "            chain_type_kwargs={\"prompt\": QA_PROMPT}\n",
        "        )\n",
        "\n",
        "        print(f\"Querying LLM with (via chain): '{query}'\")\n",
        "        result = qa_chain.invoke({\"query\": query}) # Langchain chains use 'invoke'\n",
        "\n",
        "        print(f\"\\nLLM Answer for '{query}':\")\n",
        "        print(f\"Answer: {result['result']}\")\n",
        "\n",
        "        print(\"\\nSource Documents considered by LLM:\")\n",
        "        for i, doc in enumerate(result[\"source_documents\"]):\n",
        "            print(f\"  Source {i+1} (Metadata: {doc.metadata}): {doc.page_content[:100].strip()}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during RetrievalQA chain execution or LLM call: {e}\")\n",
        "\n",
        "    print(\"\\n--- Langchain RAG with OpenRouter via LiteLLM Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpCtrsUM-Xcx",
        "outputId": "083ba212-97ff-4ae1-c9c5-575c279fe707"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Langchain RAG with OpenRouter via LiteLLM ---\n",
            "OpenRouter API Key loaded from Colab Secrets.\n",
            "\n",
            "Configuring LLM: Langchain LiteLLM Wrapper with model 'openrouter/openai/gpt-3.5-turbo'\n",
            "LLM configured.\n",
            "\n",
            "Configuring Embedding Model: 'sentence-transformers/all-MiniLM-L6-v2'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-ba1202d4323d>:207: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model configured successfully.\n",
            "\n",
            "--- 1. Ingesting Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 422.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 document(s). Total characters: 379\n",
            "Split into 1 chunks.\n",
            "\n",
            "--- 2. Storing in Vector Database (ChromaDB) ---\n",
            "Initializing Chroma vector store at './db_chroma_langchain_openrouter_litellm'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-10-ba1202d4323d>:244: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist() # Ensure persistence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created/loaded. Collection count (approx): 2\n",
            "\n",
            "--- 3. Performing Explicit Search (Retriever) ---\n",
            "Search query: 'What is Langchain?'\n",
            "Found 2 relevant document chunk(s):\n",
            "  Result 1 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular components for building complex chains and agents...\n",
            "  Result 2 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular components for building complex chains and agents...\n",
            "\n",
            "--- 4. Generating Answer with LLM using RetrievalQA Chain ---\n",
            "Querying LLM with (via chain): 'What is Langchain?'\n",
            "\n",
            "LLM Answer for 'What is Langchain?':\n",
            "Answer: Langchain is a framework for developing applications powered by language models.\n",
            "\n",
            "Source Documents considered by LLM:\n",
            "  Source 1 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular...\n",
            "  Source 2 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular...\n",
            "\n",
            "--- Langchain RAG with OpenRouter via LiteLLM Finished ---\n"
          ]
        }
      ]
    }
  ]
}