{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCaMb6u2wTg/irlgH3pg/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsrini7/Colabs/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-core sentence-transformers chromadb litellm --quiet"
      ],
      "metadata": {
        "id": "6AVLZwKP0vVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# langchain_rag_openrouter_litellm.py\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from typing import Any, List, Mapping, Optional, Dict, Union, cast, AsyncIterator, Iterator\n",
        "\n",
        "# --- Langchain Imports ---\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun\n",
        "from langchain_core.outputs import GenerationChunk, Generation\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# --- Other Necessary Imports ---\n",
        "import litellm\n",
        "\n",
        "# --- Configuration & Constants ---\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "DATA_DIR = \"./data_langchain\" # Use a different data directory to avoid conflicts\n",
        "SAMPLE_FILE_NAME = \"sample_langchain.txt\"\n",
        "OPENROUTER_LITELLM_MODEL_STRING = \"openrouter/openai/gpt-3.5-turbo\" # Or your preferred OpenRouter model\n",
        "DB_PATH_LANGCHAIN = './db_chroma_langchain_openrouter_litellm'\n",
        "# COLLECTION_NAME_LANGCHAIN = \"langchain_rag_openrouter_litellm\" # Chroma handles this internally based on persist_directory\n",
        "\n",
        "# --- Helper: Setup Logging (Optional) ---\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger('litellm').setLevel(logging.INFO) # To see LiteLLM logs\n",
        "\n",
        "# --- Custom Langchain LLM Class using LiteLLM ---\n",
        "class LiteLLMWrapperForLangchain(LLM):\n",
        "    \"\"\"\n",
        "    Custom Langchain LLM Wrapper for LiteLLM.\n",
        "    \"\"\"\n",
        "    model_name: str = OPENROUTER_LITELLM_MODEL_STRING\n",
        "    \"\"\"The model name to pass to litellm.completion.\"\"\"\n",
        "\n",
        "    temperature: float = 0.0\n",
        "    \"\"\"The temperature to use for the completion.\"\"\"\n",
        "\n",
        "    max_tokens: Optional[int] = 512 # Max tokens for the *output*\n",
        "    \"\"\"The maximum number of tokens to generate.\"\"\"\n",
        "\n",
        "    top_p: float = 1.0\n",
        "    \"\"\"The top-p value to use for the completion.\"\"\"\n",
        "\n",
        "    litellm_kwargs: Optional[Dict[str, Any]] = None\n",
        "    \"\"\"Additional keyword arguments to pass to litellm.completion.\"\"\"\n",
        "\n",
        "    streaming: bool = False\n",
        "    \"\"\"Whether to stream the output.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"litellm_langchain_wrapper\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"streaming\": self.streaming,\n",
        "            **(self.litellm_kwargs or {}),\n",
        "        }\n",
        "\n",
        "    def _prepare_litellm_call_kwargs(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        kwargs = self.litellm_kwargs or {}\n",
        "        kwargs[\"model\"] = self.model_name\n",
        "        kwargs[\"temperature\"] = self.temperature\n",
        "        if self.max_tokens is not None: # LiteLLM uses max_tokens for output tokens\n",
        "             kwargs[\"max_tokens\"] = self.max_tokens\n",
        "        kwargs[\"top_p\"] = self.top_p\n",
        "        if stop:\n",
        "            kwargs[\"stop\"] = stop\n",
        "        kwargs[\"api_key\"] = os.getenv(\"OPENROUTER_API_KEY\") # LiteLLM can pick this up\n",
        "        return kwargs\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call out to LiteLLM's completion endpoint.\"\"\"\n",
        "        if self.streaming: # Langchain handles streaming via _stream or _astream\n",
        "            # This _call method is for non-streaming. If streaming is true,\n",
        "            # it implies the user might have set it expecting streaming from _generate.\n",
        "            # For simplicity here, we'll just make a non-streaming call if _call is invoked.\n",
        "            # A more robust implementation would raise an error or adapt.\n",
        "            pass\n",
        "\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs) # Allow overriding with call-specific kwargs\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        response = litellm.completion(messages=messages, **call_kwargs)\n",
        "        return response.choices[0].message.content or \"\"\n",
        "\n",
        "    async def _acall(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Async call out to LiteLLM's completion endpoint.\"\"\"\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs)\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        response = await litellm.acompletion(messages=messages, **call_kwargs)\n",
        "        return response.choices[0].message.content or \"\"\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[GenerationChunk]:\n",
        "        \"\"\"Stream responses from LiteLLM.\"\"\"\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs)\n",
        "        call_kwargs[\"stream\"] = True\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        for chunk in litellm.completion(messages=messages, **call_kwargs):\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta_content = chunk.choices[0].delta.content\n",
        "                if delta_content:\n",
        "                    yield GenerationChunk(text=delta_content)\n",
        "                    if run_manager:\n",
        "                        run_manager.on_llm_new_token(delta_content)\n",
        "\n",
        "    async def _astream(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> AsyncIterator[GenerationChunk]:\n",
        "        \"\"\"Async stream responses from LiteLLM.\"\"\"\n",
        "        call_kwargs = self._prepare_litellm_call_kwargs(stop=stop)\n",
        "        call_kwargs.update(kwargs)\n",
        "        call_kwargs[\"stream\"] = True\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        async for chunk in await litellm.acompletion(messages=messages, **call_kwargs):\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                delta_content = chunk.choices[0].delta.content\n",
        "                if delta_content:\n",
        "                    yield GenerationChunk(text=delta_content)\n",
        "                    if run_manager:\n",
        "                        await run_manager.on_llm_new_token(delta_content)\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "def main():\n",
        "    print(\"--- Starting Langchain RAG with OpenRouter via LiteLLM ---\")\n",
        "\n",
        "    # 0. Setup: API Keys and Sample Data\n",
        "    # litellm.set_verbose = True # Uncomment for verbose LiteLLM logs\n",
        "\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        os.environ[\"OPENROUTER_API_KEY\"] = openrouter_api_key # For LiteLLM\n",
        "        print(\"OpenRouter API Key loaded from Colab Secrets.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ERROR: OPENROUTER_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load OpenRouter API Key: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    sample_file_path = os.path.join(DATA_DIR, SAMPLE_FILE_NAME)\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "    if not os.path.exists(sample_file_path):\n",
        "        with open(sample_file_path, \"w\") as f:\n",
        "            f.write(\"\"\"Langchain is a framework for developing applications powered by language models.\n",
        "It provides modular components for building complex chains and agents.\n",
        "Key features include document loaders, text splitters, vector stores, and LLM wrappers.\n",
        "This example uses Langchain with OpenRouter via LiteLLM for RAG.\n",
        "Retrieval Augmented Generation enhances LLM responses with external data.\n",
        "\"\"\")\n",
        "        print(f\"Created dummy sample file: '{sample_file_path}'\")\n",
        "\n",
        "    # Configure Langchain Components\n",
        "    print(f\"\\nConfiguring LLM: Langchain LiteLLM Wrapper with model '{OPENROUTER_LITELLM_MODEL_STRING}'\")\n",
        "    llm = LiteLLMWrapperForLangchain(\n",
        "        model_name=OPENROUTER_LITELLM_MODEL_STRING,\n",
        "        temperature=0.0,\n",
        "        max_tokens=256 # Max output tokens for the LLM response\n",
        "    )\n",
        "    print(\"LLM configured.\")\n",
        "\n",
        "    print(f\"\\nConfiguring Embedding Model: '{EMBED_MODEL_NAME}'\")\n",
        "    try:\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
        "        print(\"Embedding model configured successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load HuggingFace embedding model '{EMBED_MODEL_NAME}': {e}\")\n",
        "        print(\"Ensure 'pip install sentence-transformers' has been run.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 1. Ingest Text File\n",
        "    print(\"\\n--- 1. Ingesting Data ---\")\n",
        "    try:\n",
        "        # Using DirectoryLoader to load all .txt files in the directory\n",
        "        loader = DirectoryLoader(DATA_DIR, glob=f\"**/{SAMPLE_FILE_NAME}\", loader_cls=TextLoader, show_progress=True)\n",
        "        documents = loader.load()\n",
        "        if not documents:\n",
        "            print(f\"Warning: No documents loaded from '{DATA_DIR}'. Ensure '{SAMPLE_FILE_NAME}' exists.\")\n",
        "            sys.exit(1)\n",
        "        print(f\"Loaded {len(documents)} document(s). Total characters: {sum(len(doc.page_content) for doc in documents)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during document loading: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(texts)} chunks.\")\n",
        "\n",
        "    # 2. Store Contents in a Vector Database (ChromaDB)\n",
        "    print(\"\\n--- 2. Storing in Vector Database (ChromaDB) ---\")\n",
        "    try:\n",
        "        print(f\"Initializing Chroma vector store at '{DB_PATH_LANGCHAIN}'...\")\n",
        "        # If the directory exists and has data, Chroma will load it.\n",
        "        # For a fresh run, you might want to delete the DB_PATH_LANGCHAIN directory.\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=texts,\n",
        "            embedding=embeddings,\n",
        "            persist_directory=DB_PATH_LANGCHAIN\n",
        "        )\n",
        "        vectorstore.persist() # Ensure persistence\n",
        "        print(f\"Vector store created/loaded. Collection count (approx): {vectorstore._collection.count()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during vector store setup: {e}\")\n",
        "        # If error is \"Invalid dimension\" check embedding model output vs Chroma expectations.\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 3. Perform a Search Operation (via Retriever)\n",
        "    print(\"\\n--- 3. Performing Explicit Search (Retriever) ---\")\n",
        "    query = \"What is Langchain?\"\n",
        "    try:\n",
        "        # Load from disk if needed (e.g., in a separate run after ingestion)\n",
        "        # vectorstore = Chroma(persist_directory=DB_PATH_LANGCHAIN, embedding_function=embeddings)\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) # Get top 2 results\n",
        "        retrieved_docs = retriever.invoke(query) # Langchain uses 'invoke'\n",
        "\n",
        "        print(f\"Search query: '{query}'\")\n",
        "        print(f\"Found {len(retrieved_docs)} relevant document chunk(s):\")\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            print(f\"  Result {i+1} (Metadata: {doc.metadata}): {doc.page_content[:150].strip()}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during retrieval: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 4. Pass Search Results to LLM for Generating Answers (RetrievalQA Chain)\n",
        "    print(\"\\n--- 4. Generating Answer with LLM using RetrievalQA Chain ---\")\n",
        "    try:\n",
        "        # Define a prompt template (optional, but good practice)\n",
        "        prompt_template_str = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Keep the answer concise and based *only* on the provided context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "        QA_PROMPT = PromptTemplate(\n",
        "            template=prompt_template_str, input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\", # \"stuff\" puts all context into the prompt\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True, # Optionally return source documents\n",
        "            chain_type_kwargs={\"prompt\": QA_PROMPT}\n",
        "        )\n",
        "\n",
        "        print(f\"Querying LLM with (via chain): '{query}'\")\n",
        "        result = qa_chain.invoke({\"query\": query}) # Langchain chains use 'invoke'\n",
        "\n",
        "        print(f\"\\nLLM Answer for '{query}':\")\n",
        "        print(f\"Answer: {result['result']}\")\n",
        "\n",
        "        print(\"\\nSource Documents considered by LLM:\")\n",
        "        for i, doc in enumerate(result[\"source_documents\"]):\n",
        "            print(f\"  Source {i+1} (Metadata: {doc.metadata}): {doc.page_content[:100].strip()}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during RetrievalQA chain execution or LLM call: {e}\")\n",
        "\n",
        "    print(\"\\n--- Langchain RAG with OpenRouter via LiteLLM Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpCtrsUM-Xcx",
        "outputId": "083ba212-97ff-4ae1-c9c5-575c279fe707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Langchain RAG with OpenRouter via LiteLLM ---\n",
            "OpenRouter API Key loaded from Colab Secrets.\n",
            "\n",
            "Configuring LLM: Langchain LiteLLM Wrapper with model 'openrouter/openai/gpt-3.5-turbo'\n",
            "LLM configured.\n",
            "\n",
            "Configuring Embedding Model: 'sentence-transformers/all-MiniLM-L6-v2'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-ba1202d4323d>:207: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model configured successfully.\n",
            "\n",
            "--- 1. Ingesting Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 422.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 document(s). Total characters: 379\n",
            "Split into 1 chunks.\n",
            "\n",
            "--- 2. Storing in Vector Database (ChromaDB) ---\n",
            "Initializing Chroma vector store at './db_chroma_langchain_openrouter_litellm'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-10-ba1202d4323d>:244: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist() # Ensure persistence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created/loaded. Collection count (approx): 2\n",
            "\n",
            "--- 3. Performing Explicit Search (Retriever) ---\n",
            "Search query: 'What is Langchain?'\n",
            "Found 2 relevant document chunk(s):\n",
            "  Result 1 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular components for building complex chains and agents...\n",
            "  Result 2 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular components for building complex chains and agents...\n",
            "\n",
            "--- 4. Generating Answer with LLM using RetrievalQA Chain ---\n",
            "Querying LLM with (via chain): 'What is Langchain?'\n",
            "\n",
            "LLM Answer for 'What is Langchain?':\n",
            "Answer: Langchain is a framework for developing applications powered by language models.\n",
            "\n",
            "Source Documents considered by LLM:\n",
            "  Source 1 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular...\n",
            "  Source 2 (Metadata: {'source': 'data_langchain/sample_langchain.txt'}): Langchain is a framework for developing applications powered by language models.\n",
            "It provides modular...\n",
            "\n",
            "--- Langchain RAG with OpenRouter via LiteLLM Finished ---\n"
          ]
        }
      ]
    }
  ]
}