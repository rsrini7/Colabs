{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEcFcEgrzs/qNcAUHADSVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsrini7/Colabs/blob/main/PydanticAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "df1oPzcDNlph"
      },
      "outputs": [],
      "source": [
        "!pip install pydantic-ai langchain-community sentence-transformers scikit-learn numpy nest-asyncio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pydantic_ai_rag_openrouter_PYPI_VERSION.py\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# --- Pydantic-AI (from PyPI: pip install pydantic-ai) ---\n",
        "from pydantic import BaseModel, Field\n",
        "from pydantic_ai import Agent # The main class from the library\n",
        "from pydantic_ai.models.openai import OpenAIModel\n",
        "from pydantic_ai.providers.openai import OpenAIProvider\n",
        "\n",
        "# --- For Document Handling & Embeddings (Simplified) ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Configuration & Constants ---\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "DATA_DIR = \"./data_pydantic_ai_pypi\"\n",
        "SAMPLE_FILE_NAME = \"sample_pydantic_ai_pypi.txt\"\n",
        "# For pydantic-ai's OpenAIModel, we need to pass the model string OpenRouter expects\n",
        "OPENROUTER_MODEL_NAME_FOR_CLIENT = \"openai/gpt-3.5-turbo\"\n",
        "\n",
        "# --- 1. Define Pydantic Model for Structured LLM Output ---\n",
        "class RAGAnswerPyPI(BaseModel):\n",
        "    answer: str = Field(..., description=\"The concise answer to the user's question, based *only* on the provided context.\")\n",
        "    context_was_sufficient: bool = Field(..., description=\"True if the provided context was sufficient to answer the question, False otherwise.\")\n",
        "    # The pydantic-ai library might be simpler, let's keep the model less complex initially\n",
        "    # supporting_facts: Optional[List[str]] = Field(default=None, description=\"A list of key facts supporting the answer.\")\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "def main():\n",
        "    print(\"--- Starting RAG with 'pydantic-ai' (PyPI version) & OpenRouter ---\")\n",
        "\n",
        "    # 0. Setup: API Keys and Sample Data\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        # pydantic-ai's OpenAIModel will need the API key.\n",
        "        # It might also need OPENAI_API_KEY env var or accept it directly.\n",
        "        print(\"OpenRouter API Key loaded from Colab Secrets.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ERROR: OPENROUTER_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load OpenRouter API Key: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    sample_file_path = os.path.join(DATA_DIR, SAMPLE_FILE_NAME)\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "    if not os.path.exists(sample_file_path):\n",
        "        with open(sample_file_path, \"w\") as f:\n",
        "            f.write(\"\"\"The 'pydantic-ai' library from PyPI aims to provide structured outputs from LLMs.\n",
        "It uses Pydantic models to define the schema. An AI class with a configured model (e.g., OpenAIModel) is used.\n",
        "This example attempts to use it for a RAG task.\n",
        "Context is retrieved separately and then passed to the LLM via pydantic-ai for structured generation.\n",
        "OpenRouter can be used if the underlying OpenAI client used by pydantic-ai can be configured.\n",
        "AI developments include areas like machine learning and natural language processing.\n",
        "\"\"\")\n",
        "        print(f\"Created dummy sample file: '{sample_file_path}'\")\n",
        "\n",
        "    # --- Simplified RAG: Document Loading, Chunking, Embedding ---\n",
        "    print(\"\\n--- 1. Preparing Data (Load, Chunk, Embed) ---\")\n",
        "    try:\n",
        "        loader = TextLoader(sample_file_path)\n",
        "        documents = loader.load()\n",
        "        if not documents:\n",
        "            print(f\"Warning: No documents loaded from '{sample_file_path}'.\")\n",
        "            sys.exit(1)\n",
        "        print(f\"Loaded {len(documents)} document(s).\")\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=30)\n",
        "        chunks = text_splitter.split_documents(documents)\n",
        "        chunk_texts = [chunk.page_content for chunk in chunks]\n",
        "        print(f\"Split into {len(chunk_texts)} chunks.\")\n",
        "\n",
        "        print(f\"Loading embedding model: '{EMBED_MODEL_NAME}'\")\n",
        "        embed_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
        "        chunk_embeddings = np.array(embed_model.embed_documents(chunk_texts))\n",
        "        print(f\"Embedded {len(chunk_embeddings)} chunks. Embedding dim: {chunk_embeddings.shape[1]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during data preparation or embedding: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Simplified RAG: Retrieval ---\n",
        "    print(\"\\n--- 2. Performing Retrieval ---\")\n",
        "    query = \"How does the pydantic-ai library work?\"\n",
        "    try:\n",
        "        query_embedding = np.array(embed_model.embed_query(query)).reshape(1, -1)\n",
        "        similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
        "        top_k = 2\n",
        "        retrieved_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "        retrieved_context_texts = [chunk_texts[i] for i in retrieved_indices]\n",
        "        retrieved_context_combined = \"\\n---\\n\".join(retrieved_context_texts)\n",
        "\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(f\"Retrieved {len(retrieved_context_texts)} context chunk(s):\")\n",
        "        for i, idx in enumerate(retrieved_indices):\n",
        "            print(f\"  Context {i+1} (Similarity: {similarities[idx]:.4f}): {chunk_texts[idx][:150].strip()}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during retrieval: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "    # --- 3. Structured Generation with 'pydantic-ai' (PyPI version) & OpenRouter ---\n",
        "    print(\"\\n--- 3. Generating Structured Answer with 'pydantic-ai' (PyPI) ---\")\n",
        "    try:\n",
        "        # Configure the AI model for pydantic-ai\n",
        "        # We need to pass api_key and base_url to OpenAIModel if it supports it,\n",
        "        # or ensure the underlying openai client it uses is configured.\n",
        "        # Based on the (limited) pydantic-ai source, OpenAIModel takes 'api_key'\n",
        "        # and might use the standard 'OPENAI_API_BASE' env var or allow passing 'base_url'.\n",
        "\n",
        "        # Attempt 1: Pass directly to OpenAIModel\n",
        "        # This is a guess based on common patterns; pydantic-ai's docs are sparse here.\n",
        "\n",
        "        # Fallback: Set environment variable for OpenAI client if pydantic-ai uses it implicitly\n",
        "        os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "        llm_model_config = OpenAIModel(\n",
        "            model_name=OPENROUTER_MODEL_NAME_FOR_CLIENT,\n",
        "            provider=OpenAIProvider(api_key=openrouter_api_key,\n",
        "                    base_url=\"https://openrouter.ai/api/v1\",)\n",
        "        )\n",
        "\n",
        "        ai_instance = Agent(\n",
        "            model=llm_model_config,\n",
        "            response_model=RAGAnswerPyPI # The Pydantic model for the output\n",
        "        )\n",
        "\n",
        "        prompt_for_pydantic_ai = f\"\"\"\n",
        "Based *only* on the following context, answer the user's question.\n",
        "If the context is insufficient, reflect that in your answer.\n",
        "\n",
        "Context:\n",
        "---\n",
        "{retrieved_context_combined if retrieved_context_combined else \"No relevant context was found.\"}\n",
        "---\n",
        "\n",
        "User Question: {query}\n",
        "\"\"\"\n",
        "        print(f\"Using model for generation via pydantic-ai: {OPENROUTER_MODEL_NAME_FOR_CLIENT}\")\n",
        "\n",
        "        # Generate the structured response\n",
        "        # The generate method takes the prompt (which includes our context and query)\n",
        "        structured_answer_pypi = ai_instance.run_sync(prompt_for_pydantic_ai)\n",
        "\n",
        "\n",
        "        print(\"\\nStructured LLM Response (from pydantic-ai PyPI):\")\n",
        "        if structured_answer_pypi:\n",
        "            print(f\"  Answer: {structured_answer_pypi.output}\")\n",
        "            print(f\"  Usage: {structured_answer_pypi.usage()}\")\n",
        "        else:\n",
        "            print(\"  pydantic-ai did not return a structured answer.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during structured answer generation with 'pydantic-ai' (PyPI): {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n--- RAG with 'pydantic-ai' (PyPI version) Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCj7lq6uN94r",
        "outputId": "ceba5c24-83e9-43a5-9c61-901b60ad177f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting RAG with 'pydantic-ai' (PyPI version) & OpenRouter ---\n",
            "OpenRouter API Key loaded from Colab Secrets.\n",
            "\n",
            "--- 1. Preparing Data (Load, Chunk, Embed) ---\n",
            "Loaded 1 document(s).\n",
            "Split into 3 chunks.\n",
            "Loading embedding model: 'sentence-transformers/all-MiniLM-L6-v2'\n",
            "Embedded 3 chunks. Embedding dim: 384\n",
            "\n",
            "--- 2. Performing Retrieval ---\n",
            "Query: 'How does the pydantic-ai library work?'\n",
            "Retrieved 2 context chunk(s):\n",
            "  Context 1 (Similarity: 0.7035): The 'pydantic-ai' library from PyPI aims to provide structured outputs from LLMs.\n",
            "It uses Pydantic models to define the schema. An AI class with a con...\n",
            "  Context 2 (Similarity: 0.4902): Context is retrieved separately and then passed to the LLM via pydantic-ai for structured generation.\n",
            "OpenRouter can be used if the underlying OpenAI...\n",
            "\n",
            "--- 3. Generating Structured Answer with 'pydantic-ai' (PyPI) ---\n",
            "Using model for generation via pydantic-ai: openai/gpt-3.5-turbo\n",
            "\n",
            "Structured LLM Response (from pydantic-ai PyPI):\n",
            "  Answer: The pydantic-ai library works by providing structured outputs from Large Language Models (LLMs) through the use of Pydantic models to define the schema. An AI class is configured with a specific model, such as the OpenAIModel, to handle tasks like RAG tasks. It can also be used in conjunction with OpenRouter if the underlying OpenAI client used by pydantic-ai can be configured.\n",
            "  Usage: Usage(requests=1, request_tokens=160, response_tokens=86, total_tokens=246, details={'reasoning_tokens': 0, 'cached_tokens': 0})\n",
            "\n",
            "--- RAG with 'pydantic-ai' (PyPI version) Finished ---\n"
          ]
        }
      ]
    }
  ]
}