{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJI5XY6sDEjq8kZDHbGEuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsrini7/Colabs/blob/main/PydanticAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df1oPzcDNlph",
        "outputId": "0af53d61-45f8-4ff6-a885-6aeb552bafa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.5/301.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-sdk 1.16.0 requires opentelemetry-api==1.16.0, but you have opentelemetry-api 1.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pydantic-ai langchain-community sentence-transformers scikit-learn numpy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pydantic_instructor_rag_simulation_openrouter_litellm_v1.py\n",
        "from google.colab import userdata # Make sure this is at the top of your file\n",
        "import os\n",
        "# ... other imports ...\n",
        "\n",
        "def main():\n",
        "    print(\"--- Starting PydanticAI/Instructor (v1.x.x) RAG Simulation with OpenRouter via LiteLLM ---\")\n",
        "\n",
        "    # SET THE ENVIRONMENT VARIABLE HERE\n",
        "    try:\n",
        "        openrouter_api_key_value = userdata.get('OPENROUTER_API_KEY')\n",
        "        os.environ['OPENROUTER_API_KEY'] = openrouter_api_key_value\n",
        "        print(\"OpenRouter API Key loaded from Colab Secrets and set in os.environ.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ERROR: OPENROUTER_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "        return # Stop execution if key is not found\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load or set OpenRouter API Key: {e}\")\n",
        "        return # Stop execution on other errors\n",
        "\n",
        "    # ... rest of your main() function ...\n",
        "    # (Simulate Data Retrieval, Generate Structured Answer, etc.)\n",
        "    # LiteLLM, when called by instructor or the openai client, will now check\n",
        "    # os.environ['OPENROUTER_API_KEY'] for \"openrouter/...\" models."
      ],
      "metadata": {
        "id": "w6oQfoWGNrQP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !litellm --model openrouter/openai/gpt-3.5-turbo --api_base https://openrouter.ai/api/v1 --port 8000"
      ],
      "metadata": {
        "id": "soogIblxWVbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !litellm --model openrouter/openai/gpt-3.5-turbo --port 8000&"
      ],
      "metadata": {
        "id": "BY0Qwa9jNvwg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest-asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmPHJ5BWeODn",
        "outputId": "7fabbbfa-2313-4814-dcee-7bf256ac2dcd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pydantic_ai_rag_openrouter_PYPI_VERSION.py\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# --- Pydantic-AI (from PyPI: pip install pydantic-ai) ---\n",
        "from pydantic import BaseModel, Field\n",
        "from pydantic_ai import Agent # The main class from the library\n",
        "from pydantic_ai.models.openai import OpenAIModel\n",
        "from pydantic_ai.providers.openai import OpenAIProvider\n",
        "\n",
        "# --- For Document Handling & Embeddings (Simplified) ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Configuration & Constants ---\n",
        "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "DATA_DIR = \"./data_pydantic_ai_pypi\"\n",
        "SAMPLE_FILE_NAME = \"sample_pydantic_ai_pypi.txt\"\n",
        "# For pydantic-ai's OpenAIModel, we need to pass the model string OpenRouter expects\n",
        "OPENROUTER_MODEL_NAME_FOR_CLIENT = \"openai/gpt-3.5-turbo\"\n",
        "\n",
        "# --- 1. Define Pydantic Model for Structured LLM Output ---\n",
        "class RAGAnswerPyPI(BaseModel):\n",
        "    answer: str = Field(..., description=\"The concise answer to the user's question, based *only* on the provided context.\")\n",
        "    context_was_sufficient: bool = Field(..., description=\"True if the provided context was sufficient to answer the question, False otherwise.\")\n",
        "    # The pydantic-ai library might be simpler, let's keep the model less complex initially\n",
        "    # supporting_facts: Optional[List[str]] = Field(default=None, description=\"A list of key facts supporting the answer.\")\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "def main():\n",
        "    print(\"--- Starting RAG with 'pydantic-ai' (PyPI version) & OpenRouter ---\")\n",
        "\n",
        "    # 0. Setup: API Keys and Sample Data\n",
        "    try:\n",
        "        openrouter_api_key = userdata.get('OPENROUTER_API_KEY')\n",
        "        # pydantic-ai's OpenAIModel will need the API key.\n",
        "        # It might also need OPENAI_API_KEY env var or accept it directly.\n",
        "        print(\"OpenRouter API Key loaded from Colab Secrets.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"ERROR: OPENROUTER_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load OpenRouter API Key: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    sample_file_path = os.path.join(DATA_DIR, SAMPLE_FILE_NAME)\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "    if not os.path.exists(sample_file_path):\n",
        "        with open(sample_file_path, \"w\") as f:\n",
        "            f.write(\"\"\"The 'pydantic-ai' library from PyPI aims to provide structured outputs from LLMs.\n",
        "It uses Pydantic models to define the schema. An AI class with a configured model (e.g., OpenAIModel) is used.\n",
        "This example attempts to use it for a RAG task.\n",
        "Context is retrieved separately and then passed to the LLM via pydantic-ai for structured generation.\n",
        "OpenRouter can be used if the underlying OpenAI client used by pydantic-ai can be configured.\n",
        "AI developments include areas like machine learning and natural language processing.\n",
        "\"\"\")\n",
        "        print(f\"Created dummy sample file: '{sample_file_path}'\")\n",
        "\n",
        "    # --- Simplified RAG: Document Loading, Chunking, Embedding ---\n",
        "    print(\"\\n--- 1. Preparing Data (Load, Chunk, Embed) ---\")\n",
        "    try:\n",
        "        loader = TextLoader(sample_file_path)\n",
        "        documents = loader.load()\n",
        "        if not documents:\n",
        "            print(f\"Warning: No documents loaded from '{sample_file_path}'.\")\n",
        "            sys.exit(1)\n",
        "        print(f\"Loaded {len(documents)} document(s).\")\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=30)\n",
        "        chunks = text_splitter.split_documents(documents)\n",
        "        chunk_texts = [chunk.page_content for chunk in chunks]\n",
        "        print(f\"Split into {len(chunk_texts)} chunks.\")\n",
        "\n",
        "        print(f\"Loading embedding model: '{EMBED_MODEL_NAME}'\")\n",
        "        embed_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
        "        chunk_embeddings = np.array(embed_model.embed_documents(chunk_texts))\n",
        "        print(f\"Embedded {len(chunk_embeddings)} chunks. Embedding dim: {chunk_embeddings.shape[1]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during data preparation or embedding: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # --- Simplified RAG: Retrieval ---\n",
        "    print(\"\\n--- 2. Performing Retrieval ---\")\n",
        "    query = \"How does the pydantic-ai library work?\"\n",
        "    try:\n",
        "        query_embedding = np.array(embed_model.embed_query(query)).reshape(1, -1)\n",
        "        similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
        "        top_k = 2\n",
        "        retrieved_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "        retrieved_context_texts = [chunk_texts[i] for i in retrieved_indices]\n",
        "        retrieved_context_combined = \"\\n---\\n\".join(retrieved_context_texts)\n",
        "\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(f\"Retrieved {len(retrieved_context_texts)} context chunk(s):\")\n",
        "        for i, idx in enumerate(retrieved_indices):\n",
        "            print(f\"  Context {i+1} (Similarity: {similarities[idx]:.4f}): {chunk_texts[idx][:150].strip()}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during retrieval: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "    # --- 3. Structured Generation with 'pydantic-ai' (PyPI version) & OpenRouter ---\n",
        "    print(\"\\n--- 3. Generating Structured Answer with 'pydantic-ai' (PyPI) ---\")\n",
        "    try:\n",
        "        # Configure the AI model for pydantic-ai\n",
        "        # We need to pass api_key and base_url to OpenAIModel if it supports it,\n",
        "        # or ensure the underlying openai client it uses is configured.\n",
        "        # Based on the (limited) pydantic-ai source, OpenAIModel takes 'api_key'\n",
        "        # and might use the standard 'OPENAI_API_BASE' env var or allow passing 'base_url'.\n",
        "\n",
        "        # Attempt 1: Pass directly to OpenAIModel\n",
        "        # This is a guess based on common patterns; pydantic-ai's docs are sparse here.\n",
        "\n",
        "        # Fallback: Set environment variable for OpenAI client if pydantic-ai uses it implicitly\n",
        "        os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "        llm_model_config = OpenAIModel(\n",
        "            model_name=OPENROUTER_MODEL_NAME_FOR_CLIENT,\n",
        "            provider=OpenAIProvider(api_key=openrouter_api_key,\n",
        "                    base_url=\"https://openrouter.ai/api/v1\",)\n",
        "        )\n",
        "\n",
        "        ai_instance = Agent(\n",
        "            model=llm_model_config,\n",
        "            response_model=RAGAnswerPyPI # The Pydantic model for the output\n",
        "        )\n",
        "\n",
        "        prompt_for_pydantic_ai = f\"\"\"\n",
        "Based *only* on the following context, answer the user's question.\n",
        "If the context is insufficient, reflect that in your answer.\n",
        "\n",
        "Context:\n",
        "---\n",
        "{retrieved_context_combined if retrieved_context_combined else \"No relevant context was found.\"}\n",
        "---\n",
        "\n",
        "User Question: {query}\n",
        "\"\"\"\n",
        "        print(f\"Using model for generation via pydantic-ai: {OPENROUTER_MODEL_NAME_FOR_CLIENT}\")\n",
        "\n",
        "        # Generate the structured response\n",
        "        # The generate method takes the prompt (which includes our context and query)\n",
        "        structured_answer_pypi = ai_instance.run_sync(prompt_for_pydantic_ai)\n",
        "\n",
        "\n",
        "        print(\"\\nStructured LLM Response (from pydantic-ai PyPI):\")\n",
        "        if structured_answer_pypi:\n",
        "            print(f\"  Answer: {structured_answer_pypi.output}\")\n",
        "            print(f\"  Usage: {structured_answer_pypi.usage()}\")\n",
        "        else:\n",
        "            print(\"  pydantic-ai did not return a structured answer.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during structured answer generation with 'pydantic-ai' (PyPI): {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n--- RAG with 'pydantic-ai' (PyPI version) Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCj7lq6uN94r",
        "outputId": "5f40e665-7ed7-4a8b-ae81-8b270adb11cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting RAG with 'pydantic-ai' (PyPI version) & OpenRouter ---\n",
            "OpenRouter API Key loaded from Colab Secrets.\n",
            "\n",
            "--- 1. Preparing Data (Load, Chunk, Embed) ---\n",
            "Loaded 1 document(s).\n",
            "Split into 3 chunks.\n",
            "Loading embedding model: 'sentence-transformers/all-MiniLM-L6-v2'\n",
            "Embedded 3 chunks. Embedding dim: 384\n",
            "\n",
            "--- 2. Performing Retrieval ---\n",
            "Query: 'How does the pydantic-ai library work?'\n",
            "Retrieved 2 context chunk(s):\n",
            "  Context 1 (Similarity: 0.7035): The 'pydantic-ai' library from PyPI aims to provide structured outputs from LLMs.\n",
            "It uses Pydantic models to define the schema. An AI class with a con...\n",
            "  Context 2 (Similarity: 0.4902): Context is retrieved separately and then passed to the LLM via pydantic-ai for structured generation.\n",
            "OpenRouter can be used if the underlying OpenAI...\n",
            "\n",
            "--- 3. Generating Structured Answer with 'pydantic-ai' (PyPI) ---\n",
            "Using model for generation via pydantic-ai: openai/gpt-3.5-turbo\n",
            "\n",
            "Structured LLM Response (from pydantic-ai PyPI):\n",
            "  Answer: The pydantic-ai library uses Pydantic models to define schema for structured outputs from language models like LLMs. It involves creating an AI class with a configured model (such as OpenAIModel), which is then used for tasks like RAG with the help of OpenRouter if needed. The library allows for passing context to the LLMs for generating structured outputs.\n",
            "  Usage: Usage(requests=1, request_tokens=160, response_tokens=78, total_tokens=238, details={'reasoning_tokens': 0, 'cached_tokens': 0})\n",
            "\n",
            "--- RAG with 'pydantic-ai' (PyPI version) Finished ---\n"
          ]
        }
      ]
    }
  ]
}